TODO

Test with security's field filtering
Limit setting

Add mapping option to remove fields from source

This adds the `relocate_to` mapping option which will cause elasticsearch to
remove the field from the `_source` before it is saved and add it back to
`_source` when it is loaded. This saves disk space by shrinking or removing
entirely the `_source` that is saved to stored fields. On a fairly simple
http access log style index that I stole from rally's test data this cut
the index size by about 20%. I've run some experiments that make me think I
can build on this PR to cut the size of the index for these particular
document by another 20%. I expect the 40% index size reduction that I'm
seeing is about the best case scenario for this change. But I'm excited
by it anyway!

At this point the only valid values for `relocate_to` are `none` which
represents the behavior of Elasticsearch before this commit and `doc_values`
which causes Elasticsearch to remove the field from `_source` before it is
saved. When the document is loaded Elasticsearch will load the field's value
from doc values and add it back to the `_source`.

There are whole bunch of times when you can't use this feature, all of which
will result in an error sent back to the user:
* Fields inside of objects can't be rebuilt correctly so they can't declare
`relocate_to` at all. If they do then the mapping update will return an error.
* Fields without `doc_values` enabled can't be relocated to doc values so
any mapping update that would create a field like that will fail.
* `relocate_to` can't be changed from `doc_values` to `none` because
Elasticsearch wouldn't be able to rebuild the `_source` field documents
indexed when `relocate_to` was `doc_values`. Any mapping update that makes
this change will fail.
* Not all field types support relocation. In some case this is because we just
haven't written the code for them yet. In other cases this is because no such
code is possible. Either way, attempting to configure relocation on these
fields will fail to update the mapping.
* If a field is configured to be relocated but is sent to Elasticsearch as a
list then Elasticsearch will reject the document. This is because doc values
doesn't store the difference between a singleton list and a single valued
field. Or an empty list and a missing field. Additionally, most doc values
implementations sort the values so we'd lose the original order of the list.
So, for simplicity, we only support `relocate_to` for single valued fields.
* Fields cannot set both `relocate_to` to `doc_values` and `ignore_malformed`
to `true` because malformed values wouldn't be saved in doc values and would
vanish on load which just feels like a foot gun.

There are some caveats to this feature as well:
* Explicit `null` values of relocated fields will be removed from the
`_source` entirely. Unless they use `null_value`. In that case they'll come
back as the configured `null_value`.
* Dates will always come back in their configured format regardless of what
format they had in the original `_source`.
* IP address will always come back normalized regardless of how they were
originally sent.
* Numbers will always come back as numbers, even if they were strings in the
original source.
* Numbers may lose precision if we lost precision storing them in doc values.

This feature slows down indexing to about the same degree that configuring
`includes` or `excludes` on the `_source` field does. I believe I can
significantly improve this in a followup but I do want to for a followup
because there is enough to this change already. I do think that this will
be one of my first follow ups after this change because my understanding
is that the folks who care about saving disk space also care a lot about
indexing speed. But I'd really like to have some rally benchmarks that I
I can point to to know for sure how much performance improbement I'm getting.

This feature also slows down loading the `_source` during the fetch phase
but I expect this is in most cases less important than the slow down at
index time. The slow down is probably greater, but my understanding is that
folks that care the most about saving disk space do mostly aggregations which
should be unaffected by this change. Still, there is a fair bit that I can
do to improve the fetch phase speed when this feature is enabled. I just
want to wait on it until I can get the some benchmarks showing just *how*
slow this is. Saving space on disk means that less data has to be loaded
from disk which sounds great for performance but instead of all of the data
to load being stored together it is scattered into doc values which in
organized per field. My intuition is that the performance characteristics
of this feature are complicated.

I am 99% sure that this change has no performance impact if the feature isn't
enabled.

This change only adds this feature to numeric, date, and ip fields. I limited
the number of fields to keep the PR from getting even bigger than it already
is. I chose those particular fields because the test data that I was using
was some http access logs from during the world cup. They had four sorts of
fields: numberic, date, ip, and keyword. I've experimented with `keyword`
fields and that is how I got the 40% disk size reduction, but the rules are
fairly complicated so I want to save implementing them for a follow up change.
